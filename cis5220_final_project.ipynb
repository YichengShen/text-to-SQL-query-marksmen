{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YichengShen/cis5220-project/blob/main/cis5220_final_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ffOLBkDpqjR"
      },
      "source": [
        "# Text-to-SQL\n",
        "\n",
        "Team: Query Marksman"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_49X02COwghG"
      },
      "source": [
        "## Section 0: Config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L56K90IoxSxs"
      },
      "source": [
        "Flags for choosing which model to run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OqfxZmx-wgAk"
      },
      "outputs": [],
      "source": [
        "RUN_NON_DL_MODEL = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fmawY3UxX8a"
      },
      "source": [
        "Seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_UXyk4tLxQVc"
      },
      "outputs": [],
      "source": [
        "SEED = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtUZpHLpqAp6"
      },
      "source": [
        "## Section 1: Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D02_sQ2Mu96Y"
      },
      "source": [
        "### Install & imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVoBWIzhp6wS",
        "outputId": "f928231f-c3d5-4811-99be-4e5c2fc37ae3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk) (1.1.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk) (4.65.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rvD19zjaqdYh"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import subprocess\n",
        "\n",
        "import re\n",
        "import io\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import nltk\n",
        "from typing import List, Dict, Tuple, Any, Union"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLmI08o_xCCi",
        "outputId": "03f64e2d-b084-4f6c-908e-269a5698fd34"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbzCHSKXvEx_"
      },
      "source": [
        "Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcA3fpvPocVp",
        "outputId": "cb8c1a4b-2149-4976-da69-3223ee804f81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Z2L8fiuxiI3"
      },
      "source": [
        "### Load data into Colab notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qy5rScgzvIPW"
      },
      "source": [
        "Before you run the code below, make sure you download the Spider dataset from [here](https://yale-lily.github.io/spider). Then, you upload the zip file of the dataset to your Drive.\n",
        "\n",
        "Copy data from Drive into the current runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nNvmyuljvKIc",
        "outputId": "d160590c-7b5e-46f3-d59c-37536172ea4e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/data/spider.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# Create data folder if not exist\n",
        "!mkdir -p data\n",
        "\n",
        "# Change this path to where you store spider.zip in your Drive\n",
        "dataset_zip_path_in_drive = \"/content/drive/MyDrive/CIS5220_final_project/spider.zip\"\n",
        "dataset_zip_path_in_runtime = \"/content/data/spider.zip\"\n",
        "\n",
        "shutil.copy(dataset_zip_path_in_drive, dataset_zip_path_in_runtime)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7Bpo8LGxLXe"
      },
      "source": [
        "Unzip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "x5tYlTF3xFMK"
      },
      "outputs": [],
      "source": [
        "!unzip -q -o /content/data/spider.zip -d /content/data/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Python scripts"
      ],
      "metadata": {
        "id": "7K60v2upuovD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scripts_path_in_drive = \"/content/drive/MyDrive/CIS5220_final_project/scripts\"\n",
        "scripts_path_in_runtime = \"/content/scripts\"\n",
        "\n",
        "# Overrides previous scripts folder\n",
        "if os.path.exists(scripts_path_in_runtime):\n",
        "    shutil.rmtree(scripts_path_in_runtime)\n",
        "shutil.copytree(scripts_path_in_drive, scripts_path_in_runtime)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Su4uwENLutAx",
        "outputId": "27363ad0-7bae-48b1-a671-47e1b6aedb24"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/scripts'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G43PL2X2K-Xb"
      },
      "source": [
        "## Section 2: Data Preparation & Cleaning (Milestone 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ik-NAtpULHrz"
      },
      "source": [
        "### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "WGpYm9-hueH4"
      },
      "outputs": [],
      "source": [
        "def process(sql_data: List[Dict], \n",
        "            table_data: List[Dict]) -> Tuple[List[Dict], Dict[str, Dict]]:\n",
        "    output_tab = {}\n",
        "    for i in range(len(table_data)):\n",
        "      table = table_data[i]\n",
        "      temp = {}\n",
        "      temp['col_map'] = table['column_names']\n",
        "\n",
        "      db_name = table['db_id']\n",
        "      output_tab[db_name] = temp\n",
        "\n",
        "\n",
        "    output_sql = []\n",
        "    for i in range(len(sql_data)):\n",
        "      sql = sql_data[i]\n",
        "      temp = {}\n",
        "\n",
        "      # add query metadata\n",
        "      temp['question'] = sql['question']\n",
        "      temp['question_tok'] = sql['question_toks']\n",
        "      temp['query'] = sql['query'].replace('\\t', '') # Remove \\t, this affects generating the label file\n",
        "      temp['query_tok'] = sql['query_toks']\n",
        "      temp['table_id'] = sql['db_id']\n",
        "      sql_temp = {}\n",
        "\n",
        "      # process agg/sel\n",
        "      sql_temp['agg'] = []\n",
        "      sql_temp['sel'] = []\n",
        "      gt_sel = sql['sql']['select'][1]\n",
        "      for tup in gt_sel:\n",
        "        sql_temp['agg'].append(tup[0])\n",
        "        sql_temp['sel'].append(tup[1][1][1])\n",
        "      \n",
        "      # process where conditions and conjuctions\n",
        "      sql_temp['cond'] = []\n",
        "      gt_cond = sql['sql']['where']\n",
        "      if len(gt_cond) > 0:\n",
        "        conds = [gt_cond[x] for x in range(len(gt_cond)) if x % 2 == 0]\n",
        "        for cond in conds:\n",
        "          curr_cond = []\n",
        "          curr_cond.append(cond[2][1][1])\n",
        "          curr_cond.append(cond[1])\n",
        "          if cond[4] is not None:\n",
        "            curr_cond.append([cond[3], cond[4]])\n",
        "          else:\n",
        "            curr_cond.append(cond[3])\n",
        "          sql_temp['cond'].append(curr_cond)\n",
        "\n",
        "      sql_temp['conj'] = [gt_cond[x] for x in range(len(gt_cond)) if x % 2 == 1]\n",
        "\n",
        "      # process group by / having\n",
        "      sql_temp['group'] = [x[1] for x in sql['sql']['groupBy']]\n",
        "      having_cond = []\n",
        "      if len(sql['sql']['having']) > 0:\n",
        "        gt_having = sql['sql']['having'][0] # currently only do first having condition\n",
        "        having_cond.append(gt_having[2][1][0]) # aggregator\n",
        "        having_cond.append(gt_having[2][1][1]) # column\n",
        "        having_cond.append(gt_having[1]) # operator\n",
        "        if gt_having[4] is not None:\n",
        "          having_cond.append([gt_having[3], gt_having[4]])\n",
        "        else:\n",
        "          having_cond.append(gt_having[3])\n",
        "      sql_temp['group'].append(having_cond)\n",
        "\n",
        "      # process order by / limit\n",
        "      order_aggs = []\n",
        "      order_cols = []\n",
        "      order_par = -1\n",
        "      gt_order = sql['sql']['orderBy']\n",
        "      if len(gt_order) > 0:\n",
        "        order_aggs = [x[1][0] for x in gt_order[1]]\n",
        "        order_cols = [x[1][1] for x in gt_order[1]]\n",
        "        order_par = 1 if gt_order[0] == 'asc' else 0\n",
        "      sql_temp['order'] = [order_aggs, order_cols, order_par]\n",
        "\n",
        "      # process intersect/except/union\n",
        "      sql_temp['special'] = 0\n",
        "      if sql['sql']['intersect'] is not None:\n",
        "        sql_temp['special'] = 1\n",
        "      elif sql['sql']['except'] is not None:\n",
        "        sql_temp['special'] = 2\n",
        "      elif sql['sql']['union'] is not None:\n",
        "        sql_temp['special'] = 3\n",
        "\n",
        "      temp['sql'] = sql_temp\n",
        "      output_sql.append(temp)\n",
        "    return output_sql, output_tab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RgYlGa8xr0fT"
      },
      "outputs": [],
      "source": [
        "def load_data_new(sql_paths: Union[str, List[str]], \n",
        "                  table_paths: Union[str, List[str]], \n",
        "                  use_small: bool = False) -> Tuple[List[Dict], Dict[str, Dict]]:\n",
        "    if not isinstance(sql_paths, list):\n",
        "        sql_paths = (sql_paths, )\n",
        "    if not isinstance(table_paths, list):\n",
        "        table_paths = (table_paths, )\n",
        "    sql_data = []\n",
        "    table_data = {}\n",
        "    for i, SQL_PATH in enumerate(sql_paths):\n",
        "        if use_small and i >= 2:\n",
        "            break\n",
        "        print(f\"Loading data from {SQL_PATH}\")\n",
        "        with open(SQL_PATH) as inf:\n",
        "            data = json.load(inf)\n",
        "            sql_data += data\n",
        "                \n",
        "    for i, TABLE_PATH in enumerate(table_paths):\n",
        "        if use_small and i >= 2:\n",
        "            break\n",
        "        print(f\"Loading data from {TABLE_PATH}\")\n",
        "        with open(TABLE_PATH) as inf:\n",
        "            table_data= json.load(inf)\n",
        "    # print sql_data[0]\n",
        "    sql_data, table_data = process(sql_data, table_data)\n",
        "    return sql_data, table_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVmnWaHEuxlz"
      },
      "source": [
        "### Load Clean Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evSVkVrRuzZz",
        "outputId": "4c44e391-bf07-4ee3-c71e-039a657d61f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from /content/data/spider/train_spider.json\n",
            "Loading data from /content/data/spider/tables.json\n"
          ]
        }
      ],
      "source": [
        "sql_data_train, table_data = load_data_new([\"/content/data/spider/train_spider.json\"], [\"/content/data/spider/tables.json\"], use_small=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqCDNCxvpP7Q",
        "outputId": "701e2885-5d60-46be-c23c-d41e73bd73cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from /content/data/spider/dev.json\n",
            "Loading data from /content/data/spider/tables.json\n"
          ]
        }
      ],
      "source": [
        "sql_data_dev, table_data = load_data_new([\"/content/data/spider/dev.json\"], [\"/content/data/spider/tables.json\"], use_small=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klEYyJoOMLDC"
      },
      "source": [
        "## Section 3: EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pk99_xlhdNUg"
      },
      "source": [
        "### SQL Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3LWEdt6N8KC",
        "outputId": "e60a2e09-d89c-46fc-ff80-0ff67736e981"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training data: 7000\n",
            "Number of eval data: 1034\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of training data: {len(sql_data_train)}\")\n",
        "print(f\"Number of eval data: {len(sql_data_dev)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0_t2aN4MXeq"
      },
      "source": [
        "One example looks like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mIbghp11CAa",
        "outputId": "3d06d988-19e2-498c-8cb6-afa8bfd97630"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'How many heads of the departments are older than 56 ?',\n",
              " 'question_tok': ['How',\n",
              "  'many',\n",
              "  'heads',\n",
              "  'of',\n",
              "  'the',\n",
              "  'departments',\n",
              "  'are',\n",
              "  'older',\n",
              "  'than',\n",
              "  '56',\n",
              "  '?'],\n",
              " 'query': 'SELECT count(*) FROM head WHERE age  >  56',\n",
              " 'query_tok': ['SELECT',\n",
              "  'count',\n",
              "  '(',\n",
              "  '*',\n",
              "  ')',\n",
              "  'FROM',\n",
              "  'head',\n",
              "  'WHERE',\n",
              "  'age',\n",
              "  '>',\n",
              "  '56'],\n",
              " 'table_id': 'department_management',\n",
              " 'sql': {'agg': [3],\n",
              "  'sel': [0],\n",
              "  'cond': [[10, 3, 56.0]],\n",
              "  'conj': [],\n",
              "  'group': [[]],\n",
              "  'order': [[], [], -1],\n",
              "  'special': 0}}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "sql_data_train[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WduOxRV_dQ1w"
      },
      "source": [
        "### Database Schema Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jd7W9pLxds7R",
        "outputId": "9175de1d-8d0b-4584-9104-eba6993a1dea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "166"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "len(table_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ewbMHqh1DwV",
        "outputId": "f3acb838-c048-4e61-f3dc-9cb5b0ec58d8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'col_map': [[-1, '*'],\n",
              "  [0, 'bid'],\n",
              "  [0, 'business id'],\n",
              "  [0, 'name'],\n",
              "  [0, 'full address'],\n",
              "  [0, 'city'],\n",
              "  [0, 'latitude'],\n",
              "  [0, 'longitude'],\n",
              "  [0, 'review count'],\n",
              "  [0, 'is open'],\n",
              "  [0, 'rating'],\n",
              "  [0, 'state'],\n",
              "  [1, 'id'],\n",
              "  [1, 'business id'],\n",
              "  [1, 'category name'],\n",
              "  [2, 'uid'],\n",
              "  [2, 'user id'],\n",
              "  [2, 'name'],\n",
              "  [3, 'cid'],\n",
              "  [3, 'business id'],\n",
              "  [3, 'count'],\n",
              "  [3, 'day'],\n",
              "  [4, 'id'],\n",
              "  [4, 'business id'],\n",
              "  [4, 'neighbourhood name'],\n",
              "  [5, 'rid'],\n",
              "  [5, 'business id'],\n",
              "  [5, 'user id'],\n",
              "  [5, 'rating'],\n",
              "  [5, 'text'],\n",
              "  [5, 'year'],\n",
              "  [5, 'month'],\n",
              "  [6, 'tip id'],\n",
              "  [6, 'business id'],\n",
              "  [6, 'text'],\n",
              "  [6, 'user id'],\n",
              "  [6, 'likes'],\n",
              "  [6, 'year'],\n",
              "  [6, 'month']]}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "table_data['yelp']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 4: Code for Evaluation"
      ],
      "metadata": {
        "id": "umohFnf6x69d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(preds_file, labels_file, evaluation_type=\"all\", \n",
        "             database_dir=\"./data/spider/database\", \n",
        "             table_file=\"./data/spider/tables.json\",\n",
        "             verbose=\"False\"):\n",
        "    \"\"\"\n",
        "    Runs the evaluation script for the Spider dataset using the provided labels and predictions files.\n",
        "    It prints the evaluation results to the console and returns the subprocess result object.\n",
        "\n",
        "    Args:\n",
        "        preds_file (str): Path to the predictions file. In this file, each line is `a ground-truth SQL \\t db_id`.\n",
        "        labels_file (str): Path to the labels (gold) file. In this file, each line is a predicted SQL.\n",
        "        evaluation_type (str): Evaluation type, can be 'all', 'exec', or 'match'.\n",
        "        database_dir (str): Path to the directory containing the Spider dataset's database files.\n",
        "        table_file (str): Path to the tables.json file from the Spider dataset.\n",
        "        verbose (str): Flag to trun on or off printing details.\n",
        "\n",
        "    Returns:\n",
        "        result (subprocess.CompletedProcess): A CompletedProcess instance representing the evaluation subprocess.\n",
        "                                              It contains attributes like 'stdout' and 'stderr' to access the output\n",
        "                                              and error messages respectively.\n",
        "    \"\"\"\n",
        "\n",
        "    cmd = [\n",
        "        \"python3\", \"scripts/evaluation.py\",\n",
        "        \"--gold\", labels_file,\n",
        "        \"--pred\", preds_file,\n",
        "        \"--etype\", evaluation_type,\n",
        "        \"--db\", database_dir,\n",
        "        \"--table\", table_file,\n",
        "        \"--verbose\", verbose\n",
        "    ]\n",
        "\n",
        "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
        "\n",
        "    print(result.stdout)\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "um6ihVWO0eeG"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lfh4mCersVSB"
      },
      "source": [
        "## Section 5: Non-DL Model (Milestone 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "fEIRwW03u-9H"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest Model"
      ],
      "metadata": {
        "id": "bV9ddrV76pgn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "mOZKi9rUwduz"
      },
      "outputs": [],
      "source": [
        "class RF_TextToSQL(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.vectorizer = HashingVectorizer(n_features=2**16)\n",
        "        self.model = RandomForestClassifier(n_estimators=60)\n",
        "        self.label_encoder = LabelEncoder()\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        y_encoded = self.label_encoder.fit_transform(y)\n",
        "        X_vec = self.vectorizer.transform(X)\n",
        "        self.model.fit(X_vec, y_encoded)\n",
        "\n",
        "    def predict(self, X):\n",
        "        X_vec = self.vectorizer.transform(X)\n",
        "        y_pred_encoded = self.model.predict(X_vec)\n",
        "        return self.label_encoder.inverse_transform(y_pred_encoded)\n",
        "\n",
        "    def write_predictions_to_file(self, text_questions, labels, preds_filename, labels_filename):\n",
        "        predicted_sql_queries = self.predict(text_questions)\n",
        "\n",
        "        with open(preds_filename, 'w') as output_file:\n",
        "            for pred in predicted_sql_queries:\n",
        "                output_file.write(pred + '\\n')\n",
        "\n",
        "        with open(labels_filename, 'w') as output_file:\n",
        "            for label in labels:\n",
        "                output_file.write(label + '\\n')\n",
        "                \n",
        "\n",
        "def sql_dict_to_string(sql_dict):\n",
        "    sql_string = json.dumps(sql_dict, separators=(',', ':'))\n",
        "    return sql_string\n",
        "\n",
        "def get_text_and_sql(data):\n",
        "    text = []\n",
        "    sql = []\n",
        "    sql_with_table_id = []\n",
        "\n",
        "    for item in data:\n",
        "        text.append(item['question'])\n",
        "        sql.append(item['query'])\n",
        "        sql_with_table_id.append(f\"{item['query']}\\t{item['table_id']}\")\n",
        "        # sql.append(sql_dict_to_string(item['sql']))\n",
        "\n",
        "    return text, sql, sql_with_table_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Q9WMgt-IsUOE"
      },
      "outputs": [],
      "source": [
        "if RUN_NON_DL_MODEL:\n",
        "    # Preprocess data\n",
        "    train_text, train_sql, train_sql_with_table_id = get_text_and_sql(sql_data_train)\n",
        "    dev_text, dev_sql, dev_sql_with_table_id = get_text_and_sql(sql_data_dev)\n",
        "\n",
        "    # Use a smaller subset of the dataset\n",
        "    SUBSET_SIZE = 0.99\n",
        "    _, _, train_sql_with_table_id, _ = train_test_split(train_text, train_sql_with_table_id, train_size=SUBSET_SIZE, random_state=SEED)\n",
        "    train_text, _, train_sql, _ = train_test_split(train_text, train_sql, train_size=SUBSET_SIZE, random_state=SEED)\n",
        "\n",
        "    # Train-test split\n",
        "    TEST_SIZE = 0.3\n",
        "    _, _, y_train_with_table_id, y_val_with_table_id = train_test_split(train_text, train_sql_with_table_id, test_size=TEST_SIZE, random_state=SEED)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(train_text, train_sql, test_size=TEST_SIZE, random_state=SEED)\n",
        "\n",
        "    # Initialize model\n",
        "    model = RF_TextToSQL()\n",
        "\n",
        "    # Train model\n",
        "    model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "35ISq7-_6y9W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation on the training set"
      ],
      "metadata": {
        "id": "RrFMgPFE63Gt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write predictions of training set to a file\n",
        "model.write_predictions_to_file(X_train, y_train_with_table_id, 'preds.txt', 'labels.txt')\n",
        "\n",
        "evaluation_train = evaluate(preds_file=\"preds.txt\", \n",
        "                            labels_file=\"labels.txt\", \n",
        "                            evaluation_type=\"all\", \n",
        "                            database_dir=\"./data/spider/database\", \n",
        "                            table_file=\"./data/spider/tables.json\",\n",
        "                            verbose=\"False\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLULD8x7zRAC",
        "outputId": "894833a8-1968-464c-8af4-ff82f82a99d1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     easy                 medium               hard                 extra                all                 \n",
            "count                1159                 1943                 1001                 748                  4851                \n",
            "=====================   EXECUTION ACCURACY     =====================\n",
            "execution            0.997                0.999                1.000                0.999                0.999               \n",
            "\n",
            "====================== EXACT MATCHING ACCURACY =====================\n",
            "exact match          0.997                0.999                1.000                1.000                0.999               \n",
            "\n",
            "---------------------PARTIAL MATCHING ACCURACY----------------------\n",
            "select               1.000                1.000                1.000                1.000                1.000               \n",
            "select(no AGG)       1.000                1.000                1.000                1.000                1.000               \n",
            "where                1.000                1.000                1.000                1.000                1.000               \n",
            "where(no OP)         1.000                1.000                1.000                1.000                1.000               \n",
            "group(no Having)     1.000                1.000                1.000                1.000                1.000               \n",
            "group                1.000                1.000                1.000                1.000                1.000               \n",
            "order                1.000                1.000                1.000                1.000                1.000               \n",
            "and/or               1.000                1.000                1.000                1.000                1.000               \n",
            "IUEN                 0.000                0.000                1.000                1.000                1.000               \n",
            "keywords             1.000                1.000                1.000                1.000                1.000               \n",
            "---------------------- PARTIAL MATCHING RECALL ----------------------\n",
            "select               0.997                0.999                1.000                1.000                0.999               \n",
            "select(no AGG)       0.997                0.999                1.000                1.000                0.999               \n",
            "where                1.000                1.000                1.000                1.000                1.000               \n",
            "where(no OP)         1.000                1.000                1.000                1.000                1.000               \n",
            "group(no Having)     1.000                0.998                1.000                1.000                0.999               \n",
            "group                1.000                0.998                1.000                1.000                0.999               \n",
            "order                1.000                1.000                1.000                1.000                1.000               \n",
            "and/or               1.000                1.000                1.000                1.000                1.000               \n",
            "IUEN                 0.000                0.000                1.000                1.000                1.000               \n",
            "keywords             1.000                0.999                1.000                1.000                1.000               \n",
            "---------------------- PARTIAL MATCHING F1 --------------------------\n",
            "select               0.999                1.000                1.000                1.000                1.000               \n",
            "select(no AGG)       0.999                1.000                1.000                1.000                1.000               \n",
            "where                1.000                1.000                1.000                1.000                1.000               \n",
            "where(no OP)         1.000                1.000                1.000                1.000                1.000               \n",
            "group(no Having)     1.000                0.999                1.000                1.000                1.000               \n",
            "group                1.000                0.999                1.000                1.000                1.000               \n",
            "order                1.000                1.000                1.000                1.000                1.000               \n",
            "and/or               1.000                1.000                1.000                1.000                1.000               \n",
            "IUEN                 1.000                1.000                1.000                1.000                1.000               \n",
            "keywords             1.000                1.000                1.000                1.000                1.000               \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation on the test set"
      ],
      "metadata": {
        "id": "lrs-0PXa66YX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.write_predictions_to_file(dev_text, dev_sql_with_table_id, 'preds.txt', 'labels.txt')\n",
        "\n",
        "evaluation_test = evaluate(preds_file=\"preds.txt\", \n",
        "                          labels_file=\"labels.txt\", \n",
        "                          evaluation_type=\"all\", \n",
        "                          database_dir=\"./data/spider/database\", \n",
        "                          table_file=\"./data/spider/tables.json\",\n",
        "                          verbose=\"False\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6l5YA1mb4l2L",
        "outputId": "95383135-f01a-4313-865b-ba7cca60d894"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     easy                 medium               hard                 extra                all                 \n",
            "count                248                  446                  174                  166                  1034                \n",
            "=====================   EXECUTION ACCURACY     =====================\n",
            "execution            0.012                0.000                0.000                0.000                0.003               \n",
            "\n",
            "====================== EXACT MATCHING ACCURACY =====================\n",
            "exact match          0.012                0.000                0.000                0.000                0.003               \n",
            "\n",
            "---------------------PARTIAL MATCHING ACCURACY----------------------\n",
            "select               0.556                0.091                0.000                0.333                0.269               \n",
            "select(no AGG)       0.556                0.091                0.000                0.333                0.269               \n",
            "where                0.000                0.000                0.000                0.000                0.000               \n",
            "where(no OP)         0.000                0.000                0.000                0.000                0.000               \n",
            "group(no Having)     0.000                0.000                0.000                0.000                0.000               \n",
            "group                0.000                0.000                0.000                0.000                0.000               \n",
            "order                0.000                0.000                0.000                0.000                0.000               \n",
            "and/or               1.000                0.899                0.897                0.880                0.920               \n",
            "IUEN                 0.000                0.000                0.000                0.000                0.000               \n",
            "keywords             0.500                0.286                0.000                0.000                0.200               \n",
            "---------------------- PARTIAL MATCHING RECALL ----------------------\n",
            "select               0.020                0.002                0.000                0.006                0.007               \n",
            "select(no AGG)       0.020                0.002                0.000                0.006                0.007               \n",
            "where                0.000                0.000                0.000                0.000                0.000               \n",
            "where(no OP)         0.000                0.000                0.000                0.000                0.000               \n",
            "group(no Having)     0.000                0.000                0.000                0.000                0.000               \n",
            "group                0.000                0.000                0.000                0.000                0.000               \n",
            "order                0.000                0.000                0.000                0.000                0.000               \n",
            "and/or               1.000                1.000                1.000                1.000                1.000               \n",
            "IUEN                 0.000                0.000                0.000                0.000                0.000               \n",
            "keywords             0.007                0.005                0.000                0.000                0.003               \n",
            "---------------------- PARTIAL MATCHING F1 --------------------------\n",
            "select               0.039                0.004                1.000                0.012                0.013               \n",
            "select(no AGG)       0.039                0.004                1.000                0.012                0.013               \n",
            "where                1.000                1.000                1.000                1.000                1.000               \n",
            "where(no OP)         1.000                1.000                1.000                1.000                1.000               \n",
            "group(no Having)     1.000                1.000                1.000                1.000                1.000               \n",
            "group                1.000                1.000                1.000                1.000                1.000               \n",
            "order                1.000                1.000                1.000                1.000                1.000               \n",
            "and/or               1.000                0.947                0.945                0.936                0.958               \n",
            "IUEN                 1.000                1.000                1.000                1.000                1.000               \n",
            "keywords             0.013                0.010                1.000                1.000                0.007               \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HLpS-l996Qn3"
      },
      "execution_count": 23,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1vE7O/rAmPuf6vpgWvIpC",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}