{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YichengShen/cis5220-project/blob/main/gpt3.5_turbo_with_db.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-3.5-turbo"
      ],
      "metadata": {
        "id": "HukEy8xLgVgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install gpt_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQguFwZQ0CfL",
        "outputId": "48a8b898-d60f-46a6-94bd-753c8396b415"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.9/dist-packages (0.0.141)\n",
            "Requirement already satisfied: gptcache>=0.1.7 in /usr/local/lib/python3.9/dist-packages (from langchain) (0.1.11)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from langchain) (4.0.2)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.9/dist-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /usr/local/lib/python3.9/dist-packages (from langchain) (1.2.4)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.9/dist-packages (from langchain) (3.8.4)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.9/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain) (1.22.4)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain) (1.10.7)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.9/dist-packages (from langchain) (2.27.1)\n",
            "Requirement already satisfied: SQLAlchemy<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain) (1.4.47)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.9/dist-packages (from langchain) (0.5.7)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.2)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /usr/local/lib/python3.9/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
            "Requirement already satisfied: typing-inspect>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.8.0)\n",
            "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /usr/local/lib/python3.9/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.9/dist-packages (from gptcache>=0.1.7->langchain) (5.3.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.9/dist-packages (from gptcache>=0.1.7->langchain) (0.27.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic<2,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2->langchain) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/dist-packages (from SQLAlchemy<2,>=1->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.9/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai->gptcache>=0.1.7->langchain) (4.65.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gpt_index in /usr/local/lib/python3.9/dist-packages (0.5.15)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from gpt_index) (1.22.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from gpt_index) (1.5.3)\n",
            "Requirement already satisfied: openai>=0.26.4 in /usr/local/lib/python3.9/dist-packages (from gpt_index) (0.27.4)\n",
            "Requirement already satisfied: langchain>=0.0.123 in /usr/local/lib/python3.9/dist-packages (from gpt_index) (0.0.141)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.9/dist-packages (from gpt_index) (0.5.7)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.9/dist-packages (from gpt_index) (8.2.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.9/dist-packages (from gpt_index) (0.3.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from langchain>=0.0.123->gpt_index) (4.0.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.9/dist-packages (from langchain>=0.0.123->gpt_index) (2.27.1)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.9/dist-packages (from langchain>=0.0.123->gpt_index) (3.8.4)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.9/dist-packages (from langchain>=0.0.123->gpt_index) (6.0)\n",
            "Requirement already satisfied: SQLAlchemy<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain>=0.0.123->gpt_index) (1.4.47)\n",
            "Requirement already satisfied: gptcache>=0.1.7 in /usr/local/lib/python3.9/dist-packages (from langchain>=0.0.123->gpt_index) (0.1.11)\n",
            "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /usr/local/lib/python3.9/dist-packages (from langchain>=0.0.123->gpt_index) (1.2.4)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain>=0.0.123->gpt_index) (1.10.7)\n",
            "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /usr/local/lib/python3.9/dist-packages (from dataclasses-json->gpt_index) (1.5.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /usr/local/lib/python3.9/dist-packages (from dataclasses-json->gpt_index) (3.19.0)\n",
            "Requirement already satisfied: typing-inspect>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from dataclasses-json->gpt_index) (0.8.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai>=0.26.4->gpt_index) (4.65.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->gpt_index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->gpt_index) (2022.7.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.9/dist-packages (from tiktoken->gpt_index) (2022.10.31)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.123->gpt_index) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.123->gpt_index) (1.8.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.123->gpt_index) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.123->gpt_index) (22.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.123->gpt_index) (2.0.12)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.123->gpt_index) (1.3.3)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.9/dist-packages (from gptcache>=0.1.7->langchain>=0.0.123->gpt_index) (5.3.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.9/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json->gpt_index) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic<2,>=1->langchain>=0.0.123->gpt_index) (4.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->gpt_index) (1.16.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2->langchain>=0.0.123->gpt_index) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2->langchain>=0.0.123->gpt_index) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2->langchain>=0.0.123->gpt_index) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/dist-packages (from SQLAlchemy<2,>=1->langchain>=0.0.123->gpt_index) (2.0.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from typing-inspect>=0.4.0->dataclasses-json->gpt_index) (1.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGncUm6Q0N_3",
        "outputId": "765927bb-23fd-4486-b014-aa1670268d9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1Lixi-6y-_G"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.schema import BaseLanguageModel\n",
        "from sqlalchemy import create_engine\n",
        "from tqdm import tqdm\n",
        "\n",
        "from gpt_index import GPTSQLStructStoreIndex, LLMPredictor, SQLDatabase, ServiceContext"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = '' # You need to add an API key to run this code."
      ],
      "metadata": {
        "id": "B5DPvcjw5hHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data folder if not exist\n",
        "!mkdir -p data\n",
        "\n",
        "# Change this path to where you store spider.zip in your Drive\n",
        "dataset_zip_path_in_drive = \"/content/drive/Shareddrives/CIS 522/spider.zip\"\n",
        "dataset_zip_path_in_runtime = \"/content/data/spider.zip\"\n",
        "\n",
        "shutil.copy(dataset_zip_path_in_drive, dataset_zip_path_in_runtime)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "m5j3oovr4CmN",
        "outputId": "6720bfd0-f327-479e-b778-4d0a462ac10b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/data/spider.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q -o /content/data/spider.zip -d /content/data/"
      ],
      "metadata": {
        "id": "RNbePOrb4KWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.getLogger(\"root\").setLevel(logging.WARNING)"
      ],
      "metadata": {
        "id": "HktxwJEr0UXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_spaces = re.compile(r\"\\s+\")\n",
        "_newlines = re.compile(r\"\\n+\")"
      ],
      "metadata": {
        "id": "1NaNwn9O0V8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table_paths = \"/content/data/spider/tables.json\"\n",
        "\n",
        "if not isinstance(table_paths, list):\n",
        "        table_paths = (table_paths, )\n",
        "\n",
        "for i, TABLE_PATH in enumerate(table_paths):\n",
        "    print(f\"Loading data from {TABLE_PATH}\")\n",
        "    with open(TABLE_PATH) as inf:\n",
        "        table_data= json.load(inf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "La-KlmtEtqEs",
        "outputId": "b5e1e03d-835f-4e7e-aea2-9eb1e40205e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from /content/data/spider/tables.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_dict(input_dict):\n",
        "    formatted_value = []\n",
        "\n",
        "    for i in range(len(input_dict['table_names'])):\n",
        "        table_name = input_dict['table_names'][i]\n",
        "        columns = [col[1].replace(\" \", \"_\") for col in input_dict['column_names'] if col[0] == i]\n",
        "        formatted_columns = ', '.join(columns)\n",
        "        formatted_value.append(f\"{table_name} : {formatted_columns}\")\n",
        "\n",
        "    formatted_value_str = \" | \".join(formatted_value)\n",
        "    return {input_dict['db_id']: formatted_value_str}\n",
        "\n",
        "formatted_table_data = [format_dict(d) for d in table_data]\n",
        "merged_formatted_table_data = {k: v for d in formatted_table_data for k, v in d.items()}"
      ],
      "metadata": {
        "id": "eO4R0MAstut2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _generate_sql(\n",
        "    llama_index: GPTSQLStructStoreIndex,\n",
        "    nl_query_text: str,\n",
        ") -> str:\n",
        "    \"\"\"Generate SQL query for the given NL query text.\"\"\"\n",
        "    \n",
        "    response = llama_index.query(nl_query_text)\n",
        "    if (\n",
        "        response.extra_info is None\n",
        "        or \"sql_query\" not in response.extra_info\n",
        "        or response.extra_info[\"sql_query\"] is None\n",
        "    ):\n",
        "        raise RuntimeError(\"No SQL query generated.\")\n",
        "    query = response.extra_info[\"sql_query\"]\n",
        "    # Remove newlines and extra spaces.\n",
        "    query = _newlines.sub(\" \", query)\n",
        "    query = _spaces.sub(\" \", query)\n",
        "    return query.strip()"
      ],
      "metadata": {
        "id": "eC1AU5Ri0Y-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sql(llama_indexes: dict, examples: list, output_file: str) -> None:\n",
        "    \"\"\"Generate SQL queries for the given examples and write them to the output file.\"\"\"\n",
        "    with open(output_file, \"w\") as f:\n",
        "        for example in tqdm(examples, desc=f\"Generating {output_file}\"):\n",
        "            db_name = example[\"db_id\"]\n",
        "            nl_query_text = example[\"question\"] \n",
        "            # + \" | \" + merged_formatted_table_data[db_name]\n",
        "            # print(nl_query_text)\n",
        "            # break\n",
        "            try:\n",
        "                sql_query = _generate_sql(llama_indexes[db_name], nl_query_text)\n",
        "            except Exception as e:\n",
        "                print(\n",
        "                    f\"Failed to generate SQL query for question: \"\n",
        "                    f\"{example['question']} on database: {example['db_id']}.\"\n",
        "                )\n",
        "                print(e)\n",
        "                sql_query = \"ERROR\"\n",
        "            f.write(sql_query + \"\\n\")"
      ],
      "metadata": {
        "id": "4usxBLQX0Z3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define variables or use input prompts to get values for arguments\n",
        "input_path = \"/content/data/spider\"\n",
        "output_path = \"/content/drive/Shareddrives/CIS 522/GPT3.5_pred_no_col_name\"\n",
        "model_choice = \"gpt-3.5-turbo\"  # Replace with desired model option"
      ],
      "metadata": {
        "id": "AXxlX7zr2Bma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(output_path):\n",
        "    os.makedirs(output_path)"
      ],
      "metadata": {
        "id": "BjmI0SaV2K6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Spider dataset from the input directory.\n",
        "with open(os.path.join(input_path, \"train_spider.json\"), \"r\") as f:\n",
        "    train_spider = json.load(f)\n",
        "with open(os.path.join(input_path, \"train_others.json\"), \"r\") as f:\n",
        "    train_others = json.load(f)\n",
        "with open(os.path.join(input_path, \"dev.json\"), \"r\") as f:\n",
        "    dev = json.load(f)"
      ],
      "metadata": {
        "id": "zURYTtSe0byJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create all necessary SQL database objects.\n",
        "databases = {}\n",
        "for db in train_spider + train_others + dev:\n",
        "    db_name = db[\"db_id\"]\n",
        "    if db_name in databases:\n",
        "        continue\n",
        "    db_path = os.path.join(input_path, \"database\", db_name, db_name + \".sqlite\")\n",
        "    engine = create_engine(\"sqlite:///\" + db_path)\n",
        "    databases[db_name] = (SQLDatabase(engine=engine), engine)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IF4rB5hA2tY4",
        "outputId": "c9936459-3d0c-482c-e281-019a44bd2184"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/langchain/sql_database.py:90: SAWarning: Could not instantiate type <class 'sqlalchemy.sql.sqltypes.BIGINT'> with reflected arguments ['20']; using no arguments.\n",
            "  self._metadata.reflect(\n",
            "/usr/local/lib/python3.9/dist-packages/langchain/sql_database.py:90: SAWarning: Could not instantiate type <class 'sqlalchemy.sql.sqltypes.INTEGER'> with reflected arguments ['11']; using no arguments.\n",
            "  self._metadata.reflect(\n",
            "/usr/local/lib/python3.9/dist-packages/gpt_index/langchain_helpers/sql_wrapper.py:26: SAWarning: Could not instantiate type <class 'sqlalchemy.sql.sqltypes.INTEGER'> with reflected arguments ['11']; using no arguments.\n",
            "  self.metadata_obj.reflect()\n",
            "/usr/local/lib/python3.9/dist-packages/gpt_index/langchain_helpers/sql_wrapper.py:26: SAWarning: Could not instantiate type <class 'sqlalchemy.sql.sqltypes.BIGINT'> with reflected arguments ['20']; using no arguments.\n",
            "  self.metadata_obj.reflect()\n",
            "/usr/local/lib/python3.9/dist-packages/langchain/sql_database.py:90: SAWarning: WARNING: SQL-parsed foreign key constraint '('store_id', 'store', 'store_id')' could not be located in PRAGMA foreign_keys for table staff\n",
            "  self._metadata.reflect(\n",
            "/usr/local/lib/python3.9/dist-packages/gpt_index/langchain_helpers/sql_wrapper.py:26: SAWarning: WARNING: SQL-parsed foreign key constraint '('store_id', 'store', 'store_id')' could not be located in PRAGMA foreign_keys for table staff\n",
            "  self.metadata_obj.reflect()\n",
            "/usr/local/lib/python3.9/dist-packages/langchain/sql_database.py:90: SAWarning: WARNING: SQL-parsed foreign key constraint '('Cust_ID', 'customer', 'Cust_ID')' could not be located in PRAGMA foreign_keys for table loan\n",
            "  self._metadata.reflect(\n",
            "/usr/local/lib/python3.9/dist-packages/gpt_index/langchain_helpers/sql_wrapper.py:26: SAWarning: WARNING: SQL-parsed foreign key constraint '('Cust_ID', 'customer', 'Cust_ID')' could not be located in PRAGMA foreign_keys for table loan\n",
            "  self.metadata_obj.reflect()\n",
            "/usr/local/lib/python3.9/dist-packages/langchain/sql_database.py:90: SAWarning: WARNING: SQL-parsed foreign key constraint '('Event_ID', 'Events', 'Event_ID')' could not be located in PRAGMA foreign_keys for table Assets_in_Events\n",
            "  self._metadata.reflect(\n",
            "/usr/local/lib/python3.9/dist-packages/gpt_index/langchain_helpers/sql_wrapper.py:26: SAWarning: WARNING: SQL-parsed foreign key constraint '('Event_ID', 'Events', 'Event_ID')' could not be located in PRAGMA foreign_keys for table Assets_in_Events\n",
            "  self.metadata_obj.reflect()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the LlamaIndexes for all databases.\n",
        "if model_choice in [\"gpt-3.5-turbo\", \"gpt-4\"]:\n",
        "    llm: BaseLanguageModel = ChatOpenAI(model=model_choice, temperature=0)\n",
        "else:\n",
        "    llm = OpenAI(model=model_choice, temperature=0)\n",
        "llm_predictor = LLMPredictor(llm=llm)\n",
        "llm_indexes = {}\n",
        "for db_name, (db, engine) in databases.items():\n",
        "    # Get the name of the first table in the database.\n",
        "    # This is a hack to get a table name for the index, which can use any\n",
        "    # table in the database.\n",
        "    table_name = engine.execute(\n",
        "        \"select name from sqlite_master where type = 'table'\"\n",
        "    ).fetchone()[0]\n",
        "    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n",
        "    llm_indexes[db_name] = GPTSQLStructStoreIndex.from_documents(\n",
        "        documents=[],\n",
        "        service_context=service_context,\n",
        "        sql_database=db,\n",
        "        table_name=table_name,\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWOEn-Oo2oZr",
        "outputId": "7332af98-a2dd-45b2-ca1e-6f18013d3a37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "/usr/local/lib/python3.9/dist-packages/gpt_index/langchain_helpers/sql_wrapper.py:53: SAWarning: Could not instantiate type <class 'sqlalchemy.sql.sqltypes.BIGINT'> with reflected arguments ['20']; using no arguments.\n",
            "  for column in self._inspector.get_columns(table_name):\n",
            "/usr/local/lib/python3.9/dist-packages/gpt_index/langchain_helpers/sql_wrapper.py:53: SAWarning: Could not instantiate type <class 'sqlalchemy.sql.sqltypes.INTEGER'> with reflected arguments ['11']; using no arguments.\n",
            "  for column in self._inspector.get_columns(table_name):\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "/usr/local/lib/python3.9/dist-packages/gpt_index/langchain_helpers/sql_wrapper.py:57: SAWarning: WARNING: SQL-parsed foreign key constraint '('store_id', 'store', 'store_id')' could not be located in PRAGMA foreign_keys for table staff\n",
            "  for foreign_key in self._inspector.get_foreign_keys(table_name):\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "/usr/local/lib/python3.9/dist-packages/gpt_index/langchain_helpers/sql_wrapper.py:57: SAWarning: WARNING: SQL-parsed foreign key constraint '('Cust_ID', 'customer', 'Cust_ID')' could not be located in PRAGMA foreign_keys for table loan\n",
            "  for foreign_key in self._inspector.get_foreign_keys(table_name):\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "/usr/local/lib/python3.9/dist-packages/gpt_index/langchain_helpers/sql_wrapper.py:57: SAWarning: WARNING: SQL-parsed foreign key constraint '('Event_ID', 'Events', 'Event_ID')' could not be located in PRAGMA foreign_keys for table Assets_in_Events\n",
            "  for foreign_key in self._inspector.get_foreign_keys(table_name):\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
            "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# llm_indexes\n",
        "sample_dev = dev[:3]\n",
        "sample_dev\n",
        "\n",
        "query = [a[\"query\"] for a in sample_dev]\n",
        "query"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RDRBtVaYFME",
        "outputId": "f066a5c3-134e-454a-db6d-b6a76c7fb6e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['SELECT count(*) FROM singer',\n",
              " 'SELECT count(*) FROM singer',\n",
              " 'SELECT name ,  country ,  age FROM singer ORDER BY age DESC']"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merged_formatted_table_data['perpetrator']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "X3sA0JeJtyjz",
        "outputId": "348ee1b7-c0c6-4719-eb84-cfd1647ae770"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'perpetrator : perpetrator_id, people_id, date, year, location, country, killed, injured | people : people_id, name, height, weight, home_town'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate SQL queries.\n",
        "# generate_sql(\n",
        "#     llama_indexes=llm_indexes,\n",
        "#     examples=train_spider + train_others,\n",
        "#     output_file=os.path.join(output_path, \"train_pred.sql\"),\n",
        "# )\n",
        "generate_sql(\n",
        "    llama_indexes=llm_indexes,\n",
        "    examples=dev,\n",
        "    output_file=os.path.join(output_path, \"dev_pred.sql\"),\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aJdJGtW3A6M",
        "outputId": "17ce975f-92d5-46f6-f0c4-967c548aa9a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating /content/drive/Shareddrives/CIS 522/GPT3.5_pred_no_col_name/dev_pred.sql:   9%|â–‰         | 91/1034 [02:27<24:14,  1.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to generate SQL query for question: For each continent, list its id, name, and how many countries it has? on database: car_1.\n",
            "(sqlite3.OperationalError) no such column: c.ContId\n",
            "[SQL: SELECT c.ContId, c.CountryName, COUNT(*) AS num_countries\n",
            "FROM countries c\n",
            "JOIN continents con ON c.Continent = con.ContId\n",
            "GROUP BY c.Continent\n",
            "ORDER BY num_countries DESC]\n",
            "(Background on this error at: https://sqlalche.me/e/14/e3q8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating /content/drive/Shareddrives/CIS 522/GPT3.5_pred_no_col_name/dev_pred.sql:  10%|â–‰         | 100/1034 [02:41<25:42,  1.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to generate SQL query for question: Find the name of the makers that produced some cars in the year of 1970? on database: car_1.\n",
            "(sqlite3.OperationalError) no such column: car_names.MakeId\n",
            "[SQL: SELECT DISTINCT car_makers.Maker\n",
            "FROM car_makers\n",
            "JOIN cars_data ON car_makers.Id = car_names.MakeId\n",
            "WHERE cars_data.Year = 1970\n",
            "ORDER BY car_makers.Maker ASC]\n",
            "(Background on this error at: https://sqlalche.me/e/14/e3q8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating /content/drive/Shareddrives/CIS 522/GPT3.5_pred_no_col_name/dev_pred.sql:  10%|â–‰         | 101/1034 [02:43<26:51,  1.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to generate SQL query for question: What is the name of the different car makers who produced a car in 1970? on database: car_1.\n",
            "(sqlite3.OperationalError) no such column: car_names.MakeId\n",
            "[SQL: SELECT DISTINCT car_makers.Maker\n",
            "FROM car_makers\n",
            "JOIN cars_data ON car_makers.Id = car_names.MakeId\n",
            "WHERE cars_data.Year = 1970\n",
            "ORDER BY car_makers.Maker ASC]\n",
            "(Background on this error at: https://sqlalche.me/e/14/e3q8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating /content/drive/Shareddrives/CIS 522/GPT3.5_pred_no_col_name/dev_pred.sql:  10%|â–ˆ         | 104/1034 [02:51<34:08,  2.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to generate SQL query for question: Which distinct car models are the produced after 1980? on database: car_1.\n",
            "(sqlite3.OperationalError) ambiguous column name: Model\n",
            "[SQL: SELECT DISTINCT Model\n",
            "FROM car_names\n",
            "JOIN model_list ON car_names.Model = model_list.Model\n",
            "JOIN cars_data ON car_names.MakeId = cars_data.Id\n",
            "WHERE Year > 1980\n",
            "ORDER BY Model ASC]\n",
            "(Background on this error at: https://sqlalche.me/e/14/e3q8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating /content/drive/Shareddrives/CIS 522/GPT3.5_pred_no_col_name/dev_pred.sql:  17%|â–ˆâ–‹        | 179/1034 [05:20<47:16,  3.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to generate SQL query for question: What are the ids and names of all countries that either have more than 3 car makers or produce fiat model ? on database: car_1.\n",
            "(sqlite3.OperationalError) no such column: continents.Country\n",
            "[SQL: SELECT CountryId, CountryName FROM countries\n",
            "WHERE CountryId IN (\n",
            "  SELECT Country FROM car_makers\n",
            "  GROUP BY Country\n",
            "  HAVING COUNT(DISTINCT Maker) > 3\n",
            ")\n",
            "OR CountryId IN (\n",
            "  SELECT DISTINCT continents.Country FROM continents\n",
            "  JOIN countries ON continents.ContId = countries.Continent\n",
            "  JOIN car_makers ON countries.CountryId = car_makers.Country\n",
            "  JOIN model_list ON car_makers.Id = model_list.Maker\n",
            "  WHERE model_list.Model = 'fiat'\n",
            ")\n",
            "ORDER BY CountryName]\n",
            "(Background on this error at: https://sqlalche.me/e/14/e3q8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating /content/drive/Shareddrives/CIS 522/GPT3.5_pred_no_col_name/dev_pred.sql:  21%|â–ˆâ–ˆ        | 217/1034 [05:58<21:45,  1.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to generate SQL query for question: Count the number of United Airlines flights arriving in ASY Airport. on database: flight_2.\n",
            "(sqlite3.OperationalError) no such column: airlines.Airline\n",
            "[SQL: SELECT COUNT(*) FROM flights \n",
            "JOIN airports ON flights.DestAirport = airports.AirportCode \n",
            "WHERE airlines.Airline = 'United Airlines' AND airports.AirportCode = 'ASY']\n",
            "(Background on this error at: https://sqlalche.me/e/14/e3q8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating /content/drive/Shareddrives/CIS 522/GPT3.5_pred_no_col_name/dev_pred.sql:  23%|â–ˆâ–ˆâ–Ž       | 235/1034 [06:29<21:32,  1.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to generate SQL query for question: Which airlines have a flight with source airport AHD? on database: flight_2.\n",
            "(sqlite3.OperationalError) ambiguous column name: Airline\n",
            "[SQL: SELECT Airline, Abbreviation FROM airlines\n",
            "JOIN flights ON airlines.uid = flights.Airline\n",
            "WHERE flights.SourceAirport = 'AHD'\n",
            "ORDER BY Airline ASC]\n",
            "(Background on this error at: https://sqlalche.me/e/14/e3q8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating /content/drive/Shareddrives/CIS 522/GPT3.5_pred_no_col_name/dev_pred.sql:  24%|â–ˆâ–ˆâ–Ž       | 244/1034 [06:53<32:29,  2.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to generate SQL query for question: Find all airlines that have fewer than 200 flights. on database: flight_2.\n",
            "(sqlite3.OperationalError) ambiguous column name: Airline\n",
            "[SQL: SELECT Airline, COUNT(*) as num_flights\n",
            "FROM flights\n",
            "JOIN airlines ON flights.Airline = airlines.uid\n",
            "GROUP BY Airline\n",
            "HAVING num_flights < 200\n",
            "ORDER BY num_flights ASC]\n",
            "(Background on this error at: https://sqlalche.me/e/14/e3q8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating /content/drive/Shareddrives/CIS 522/GPT3.5_pred_no_col_name/dev_pred.sql:  24%|â–ˆâ–ˆâ–Ž       | 245/1034 [06:55<31:57,  2.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to generate SQL query for question: Which airlines have less than 200 flights? on database: flight_2.\n",
            "(sqlite3.OperationalError) ambiguous column name: Airline\n",
            "[SQL: SELECT Airline, COUNT(*) as num_flights\n",
            "FROM flights\n",
            "JOIN airlines ON flights.Airline = airlines.uid\n",
            "GROUP BY Airline\n",
            "HAVING num_flights < 200\n",
            "ORDER BY num_flights ASC]\n",
            "(Background on this error at: https://sqlalche.me/e/14/e3q8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating /content/drive/Shareddrives/CIS 522/GPT3.5_pred_no_col_name/dev_pred.sql:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 456/1034 [11:25<12:02,  1.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to generate SQL query for question: List the first and last name of all players in the order of birth date. on database: wta_1.\n",
            "(sqlite3.OperationalError) Could not decode to UTF-8 column 'last_name' with text 'Treyes Albarracï¿½ï¿½N'\n",
            "(Background on this error at: https://sqlalche.me/e/14/e3q8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating /content/drive/Shareddrives/CIS 522/GPT3.5_pred_no_col_name/dev_pred.sql:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 457/1034 [11:26<11:32,  1.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to generate SQL query for question: What are the full names of all players, sorted by birth date? on database: wta_1.\n",
            "(sqlite3.OperationalError) Could not decode to UTF-8 column 'full_name' with text 'Joselyn Margarita Treyes Albarracï¿½ï¿½N'\n",
            "(Background on this error at: https://sqlalche.me/e/14/e3q8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating /content/drive/Shareddrives/CIS 522/GPT3.5_pred_no_col_name/dev_pred.sql:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 517/1034 [12:54<13:14,  1.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to generate SQL query for question: What is the name and id of the department with the most number of degrees ? on database: student_transcripts_tracking.\n",
            "(sqlite3.OperationalError) ambiguous column name: department_id\n",
            "[SQL: SELECT department_id, department_name, COUNT(degree_program_id) AS num_degrees\n",
            "FROM Degree_Programs\n",
            "JOIN Departments ON Degree_Programs.department_id = Departments.department_id\n",
            "GROUP BY department_id\n",
            "ORDER BY num_degrees DESC\n",
            "LIMIT 1;]\n",
            "(Background on this error at: https://sqlalche.me/e/14/e3q8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating /content/drive/Shareddrives/CIS 522/GPT3.5_pred_no_col_name/dev_pred.sql:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 537/1034 [13:25<18:00,  2.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to generate SQL query for question: What are the first, middle, and last names for everybody enrolled in a Bachelors program? on database: student_transcripts_tracking.\n",
            "(sqlite3.OperationalError) no such column: degree_program_id\n",
            "[SQL: SELECT first_name, middle_name, last_name \n",
            "FROM Students \n",
            "WHERE degree_program_id IN \n",
            "    (SELECT degree_program_id \n",
            "     FROM Degree_Programs \n",
            "     WHERE degree_summary_name LIKE '%Bachelor%')\n",
            "ORDER BY last_name ASC]\n",
            "(Background on this error at: https://sqlalche.me/e/14/e3q8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating /content/drive/Shareddrives/CIS 522/GPT3.5_pred_no_col_name/dev_pred.sql:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 575/1034 [14:36<12:20,  1.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to generate SQL query for question: What is the date and id of the transcript with the least number of results? on database: student_transcripts_tracking.\n",
            "(sqlite3.OperationalError) ambiguous column name: transcript_id\n",
            "[SQL: SELECT transcript_id, transcript_date, COUNT(*) as num_results\n",
            "FROM Transcript_Contents\n",
            "JOIN Transcripts ON Transcript_Contents.transcript_id = Transcripts.transcript_id\n",
            "GROUP BY Transcript_Contents.transcript_id\n",
            "ORDER BY num_results ASC\n",
            "LIMIT 1;]\n",
            "(Background on this error at: https://sqlalche.me/e/14/e3q8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating /content/drive/Shareddrives/CIS 522/GPT3.5_pred_no_col_name/dev_pred.sql:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 881/1034 [21:32<03:17,  1.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to generate SQL query for question: Show me all grades that have at least 4 students. on database: network_1.\n",
            "(sqlite3.OperationalError) no such column: student_id\n",
            "[SQL: SELECT grade, COUNT(student_id) AS num_students\n",
            "FROM Highschooler\n",
            "GROUP BY grade\n",
            "HAVING num_students >= 4\n",
            "ORDER BY grade ASC]\n",
            "(Background on this error at: https://sqlalche.me/e/14/e3q8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating /content/drive/Shareddrives/CIS 522/GPT3.5_pred_no_col_name/dev_pred.sql:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 899/1034 [22:04<03:57,  1.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to generate SQL query for question: Show the ids of high schoolers who have friends and are also liked by someone else. on database: network_1.\n",
            "(sqlite3.OperationalError) near \"Answer\": syntax error\n",
            "[SQL: SELECT DISTINCT f.student_id\n",
            "FROM Friend f\n",
            "JOIN Likes l ON f.student_id = l.liked_id\n",
            "Answer: The query returns the IDs of high schoolers who have friends and are also liked by someone else.]\n",
            "(Background on this error at: https://sqlalche.me/e/14/e3q8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating /content/drive/Shareddrives/CIS 522/GPT3.5_pred_no_col_name/dev_pred.sql:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 900/1034 [22:06<04:20,  1.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to generate SQL query for question: What are the ids of students who both have friends and are liked? on database: network_1.\n",
            "(sqlite3.OperationalError) 1st ORDER BY term does not match any column in the result set\n",
            "[SQL: SELECT DISTINCT f.student_id \n",
            "FROM Friend f \n",
            "JOIN Likes l ON f.student_id = l.student_id \n",
            "INTERSECT \n",
            "SELECT DISTINCT f.friend_id \n",
            "FROM Friend f \n",
            "JOIN Likes l ON f.friend_id = l.liked_id\n",
            "ORDER BY student_id ASC]\n",
            "(Background on this error at: https://sqlalche.me/e/14/e3q8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating /content/drive/Shareddrives/CIS 522/GPT3.5_pred_no_col_name/dev_pred.sql: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1034/1034 [25:26<00:00,  1.48s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation is done in other notebooks."
      ],
      "metadata": {
        "id": "gv8Huotrgaq0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9TruxRG4gcm9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}